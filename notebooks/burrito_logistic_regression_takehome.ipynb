{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Gvm1BZXjuBx"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IXUfiQ2UKj6"
   },
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "## Assignment üåØ\n",
    "\n",
    "Using a [**dataset of 400+ burrito reviews**](https://srcole.github.io/100burritos/), build a model that predicts whether a burrito is rated `'Great'`?\n",
    "\n",
    "## What We Want\n",
    "\n",
    "*Associate Instructor* is a teaching position where you will work closely students ‚Äî delivering curriculum, and leading question and answer sessions. The purpose of this assignment is to give us an idea of how you approach data science problems and whether you can explain that approach to someone who does not yet have your level of expertise. Given this, we are less interested you building a \"perfect\" model and more focused on how you use this assignment to teach important concepts to a data science student.\n",
    "\n",
    "## What You Need to Do\n",
    "\n",
    "- [ ] Make a copy of this notebook to work on - you can download if you have a local Jupyter setup, or click `File > Save a copy in Drive` to copy and work on with Google Colab\n",
    "- [ ] Import the burrito `csv` file into a `DataFrame`. Your target will be the `'Great'` column.\n",
    "- [ ] Conduct exploratory data analysis (EDA) to determine how you should clean the data for your pipeline.\n",
    "- [ ] Clean your data. (Note: You are not required to use all columns in your model, but justify your decisions based on your EDA.)\n",
    "- [ ] Do train/validate/test split. Train on reviews from 2016 & earlier. Validate on 2017. Test on 2018 & later.\n",
    "- [ ] Determine what the baseline accuracy is for a na√Øve classification model.\n",
    "- [ ] Create a `scikit-learn` pipeline with the following components:\n",
    "  - A one hot encoder for categorical features.\n",
    "  - A scaler.\n",
    "  - A logistic regressor.\n",
    "- [ ] Train your model using the training data.\n",
    "- [ ] Create a visualization showing your model's coefficients.\n",
    "- [ ] Get your model's validation accuracy (multiple times if you try multiple iterations).\n",
    "- [ ] Get your model's test accuracy (one time, at the end).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhHu6AF7ML3p"
   },
   "source": [
    "### Import the burrito `csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stdgqGAYjuB3"
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('https://drive.google.com/uc?export=download&id=1cctPq1sYeD6Y6mGg5Lpl-GLDJBwtdihg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4km-WYWpMoYw"
   },
   "source": [
    "### Conduct exploratory data analysis (EDA)\n",
    "\n",
    "In this section I'll take a look at the data and begin formulating ideas about which columns should be kept, deleted, or modified in some way, which I'll complete in the next section: __Clean Data__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gLoRSPwFOrg1"
   },
   "outputs": [],
   "source": [
    "# What does the data actually look like?\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data do we have?\n",
    "\n",
    "print(f'There are {df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do the columns look like?\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of data exists in each column?\n",
    "\n",
    "df.info(verbose=True)\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How balanced is the target variable?\n",
    "\n",
    "df[\"Great\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data\n",
    "\n",
    "Now I'll take what I've learned from the last section and drop rows I don't think will be useful. I'll also interpolate data as needed.\n",
    "\n",
    "The columns `Location`, `Burrito`, `Neighborhood`, and `Reviewer` represent discrete categories and can be dummy encoded. However, if there are too many unique values in each column, the number of resulting dummy columns can be huge. So first I'll check to see how many values each column has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Location\"].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Burrito\"].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Neighborhood\"].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Reviewer\"].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"NonSD\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the relatively small number of rows, it makes sense to drop these columns from consideration. Other features could be generated from these - such as combining close-by neighborhoods or clumping together burrito types (i.e. a binary feature that represents _California burrito_ vs _non-Cali_), but for time purposes and simplicity I'll skip this. In the future we can revisit these features and work with them if needed.\n",
    "\n",
    "I'm also going to __assume__ that the column `Address` and `URL` are redundant given the name of the burrito shop, so I'll drop them as well.\n",
    "\n",
    "The remaining columns are floats or objects - floats represent ratings of specific burrito parts (i.e. `Meat`) or things like `Cost` or `Weight`. The columns encoded as objects represent the absence or presence of certain features (i.e. `Avocado`) or whether or not the burrito is recommended `Rec`. `Notes` is text and can be munged, but I will skip doing that for time reasons.\n",
    "\n",
    "Also drop the `Queso` column because it's empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns I don't want\n",
    "\n",
    "df = df.drop(['Location', 'Burrito', 'Neighborhood', 'Reviewer', 'Address', 'URL', 'Notes', 'NonSD', 'Queso', 'Unreliable'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of the float columns, check which ones have lots of NaN's\n",
    "\n",
    "cols = ['Yelp', 'Google', 'Cost', 'Hunger', 'Mass (g)', 'Density (g/mL)',\n",
    "       'Length', 'Circum', 'Volume', 'Tortilla', 'Temp', 'Meat', 'Fillings',\n",
    "       'Meat:filling', 'Uniformity', 'Salsa', 'Synergy', 'Wrap']\n",
    "        \n",
    "df[cols].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns with over 100 NaN's\n",
    "\n",
    "df = df.drop(['Yelp', 'Google', 'Mass (g)', 'Density (g/mL)', 'Length', 'Circum', 'Volume'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the remaining NaN's with the column averages\n",
    "# Alternatively: drop rows with missing values\n",
    "\n",
    "cols = ['Cost', 'Hunger', 'Temp', 'Meat', 'Fillings', 'Meat:filling', 'Uniformity', 'Salsa', 'Synergy', 'Wrap']\n",
    "for col in cols:\n",
    "    df[col].fillna(value=df[col].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary encode these columns instead of dummy encoding\n",
    "\n",
    "cols = ['Beef', 'Pico', 'Guac', 'Cheese',\n",
    "       'Fries', 'Sour cream', 'Pork', 'Chicken', 'Shrimp', 'Fish', 'Rice',\n",
    "       'Beans', 'Lettuce', 'Tomato', 'Bell peper', 'Carrots', 'Cabbage',\n",
    "       'Sauce', 'Salsa.1', 'Cilantro', 'Onion', 'Taquito', 'Pineapple', 'Ham',\n",
    "       'Chile relleno', 'Nopales', 'Lobster', 'Egg', 'Mushroom',\n",
    "       'Bacon', 'Sushi', 'Avocado', 'Corn', 'Zucchini', 'Chips']\n",
    "\n",
    "d = {'X': 1, 'x': 1, np.NaN: 0}\n",
    "\n",
    "for col in cols:\n",
    "    df[col] = df[col].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime object\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "df = df.drop(\"Chips\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-r9VFXGNDYi"
   },
   "source": [
    "### Do train/validate/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex based on date\n",
    "\n",
    "df = df.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent vs dependent variables.\n",
    "\n",
    "X = [i for i in df.columns.to_list() if i not in \"Great\"]\n",
    "y = \"Great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what this looks like\n",
    "\n",
    "df[X].loc['2017-01-01':'2017-12-31'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the train/val/test sets by date\n",
    "\n",
    "X_train = df[X].loc[:'2016-12-31']\n",
    "y_train = df[y].loc[:'2016-12-31']\n",
    "\n",
    "X_val = df[X].loc['2017-01-01':'2017-12-31']\n",
    "y_val = df[y].loc['2017-01-01':'2017-12-31']\n",
    "\n",
    "X_test = df[X].loc['2018-01-01':]\n",
    "y_test = df[y].loc['2018-01-01':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many rows are in each set\n",
    "# These numbers aren't exactly balanced, but I will follow the requirements of this task and keep them as is\n",
    "\n",
    "print(f'# rows in training set:   {X_train.shape[0]}')\n",
    "print(f'# rows in validation set: {X_val.shape[0]}')\n",
    "print(f'# rows in test set:       {X_test.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoYlNeRGNJWK"
   },
   "source": [
    "### Determine baseline accuracy for a na√Øve classification model\n",
    "\n",
    "How often will a model that guesses \"great\" every time be accurate? If the distribution of Great/Not Great is very skewed (lots more greats than not greats), then we can get a high accuracy by simply classifying every burrito as \"Great\"! Some people just really love Mexican food..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Great\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{df[\"Great\"].value_counts()[1]/df.shape[0]:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract this from 1 because the model can also simply guess the opposite\n",
    "\n",
    "print(f'{1 - df[\"Great\"].value_counts()[1]/df.shape[0]:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the logistic model I create is \"useful\", its classification accuracy should be above _0.57_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ku0QDc4qssET"
   },
   "outputs": [],
   "source": [
    "# Make a simple sklearn model and get the accuracy on the training data\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Scale data\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Show accuracy on the _training_ data\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "print(f\"The model's accuracy on the training data is: {clf.score(X_train, y_train):.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3G7N-f7FNTwu"
   },
   "source": [
    "### Create a `scikit-learn` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Optional: add dummy encoding\n",
    "pipe = Pipeline([('scaler', preprocessing.StandardScaler()), \n",
    "                 ('logistic_reg', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpzhx5iJNsGs"
   },
   "source": [
    "### Train model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQ1IpBQlNrRb"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "print(f\"{pipe.score(X_test, y_test):.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaBBzwJ5N0mD"
   },
   "source": [
    "### Create visualization of model coefficients\n",
    "\n",
    "Later experiments: perform variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWXwGmjXtgu7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    clf.coef_,\n",
    "    columns=df.columns[:-1] # to remove \"Great\"\n",
    ").T\n",
    "\n",
    "coefs.plot(kind='barh', figsize=(15, 12))\n",
    "plt.title('Model coefficient size')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.subplots_adjust(left=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACl7NaeROBn3"
   },
   "source": [
    "### Get model's validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHIMBkQwut7r"
   },
   "outputs": [],
   "source": [
    "print(f\"{pipe.score(X_val, y_val):.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3xJZKOaOE1p"
   },
   "source": [
    "### Get your model's test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJ9HEVcIxJt7"
   },
   "outputs": [],
   "source": [
    "print(f\"{pipe.score(X_test, y_test):.2}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of 2020-07-13_associate-instructor-technical.ipynb",
   "provenance": [
    {
     "file_id": "1ocC-aiCTCXGIC3Ov_N7f9K92LLNxeYhl",
     "timestamp": 1622665950587
    },
    {
     "file_id": "19EpEW9L22Tco4a4Mmq-YiJLvT8LwW1Ok",
     "timestamp": 1594678209610
    },
    {
     "file_id": "1uKaoqrgBJKpjjMkBmZWHMgIN-qHVfUTy",
     "timestamp": 1594664649590
    },
    {
     "file_id": "https://github.com/LambdaSchool/DS-Unit-2-Linear-Models/blob/master/module4-logistic-regression/LS_DS_214_assignment.ipynb",
     "timestamp": 1594331617495
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
