{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification\n",
    "I'm interested in the task of _document classification_. In this notebook I try out several different techniques for document classification using the [IMDB Large Movie Review dataset](https://ai.stanford.edu/~amaas/data/sentiment/). Each row in the dataset consists of a written movie review along with the rating associated with the review (1 - 10). A positive review is defined as a review > 5. Anything lower than 5 is a negative review. I've reformulated this task as _sentiment classification_ by creating positive/negative ratings for each movie.\n",
    "\n",
    "After the normal data downloading and cleaning phase I experiment with a few different strategies:\n",
    "- V1: learning an embedding layer\n",
    "- V2: training glove embeddings on the dataset.\n",
    "- V3a: using pretrained embeddings (???)\n",
    "- V3b: using pretrained embeddings (Google universal embeddings)\n",
    "- V4:  more complicated deep learning architecture: embed sentences and documents, use CNN over documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.4.1\n",
      "Eager mode:  True\n",
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeledBow.feat \u001b[34mpos\u001b[m\u001b[m             unsupBow.feat   urls_pos.txt\n",
      "\u001b[34mneg\u001b[m\u001b[m             \u001b[34munsup\u001b[m\u001b[m           urls_neg.txt    urls_unsup.txt\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/aclImdb/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and assign to the correct class (positive or negative)\n",
    "# Ignore the pre-assigned train/test split\n",
    "\n",
    "data = []\n",
    "data_dirs = [\"../data/aclImdb/train/pos\", \"../data/aclImdb/test/pos\", \"../data/aclImdb/train/neg\", \"../data/aclImdb/test/neg\"]\n",
    "\n",
    "for dir_ in data_dirs:\n",
    "    for file in os.listdir(dir_):\n",
    "        with open(os.path.join(dir_, file), \"r\") as f:\n",
    "            lines = \"\"\n",
    "            for line in f:\n",
    "                lines += line\n",
    "\n",
    "            if \"pos\" in dir_:\n",
    "                class_ = 1\n",
    "            elif \"neg\" in dir_:\n",
    "                class_ = 0\n",
    "\n",
    "            # rating = int(file.split(\"_\")[1][0]) # we won't actually use this for the sentiment analysis task\n",
    "            data.append([class_, lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"class\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5d0d87f60809>:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>0</td>\n",
       "      <td>Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.&lt;br /&gt;&lt;br /&gt;Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>0</td>\n",
       "      <td>Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we're from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>0</td>\n",
       "      <td>Ouch! This one was a bit painful to sit through. It has a cute and amusing premise, but it all goes to hell from there. Matthew Modine is almost always pedestrian and annoying, and he does not disappoint in this one. Deborah Kara Unger and John Neville turned in surprisingly decent performances. Alan Bates and Jennifer Tilly, among others, played it way over the top. I know that's the way the parts were written, and it's hard to blame actors, when the script and director have them do such schlock. If you're going to have outrageous characters, that's OK, but you gotta have good material to make it work. It didn't here. Run away screaming from this movie if at all possible.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       class  \\\n",
       "25000  0       \n",
       "25001  0       \n",
       "25002  0       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \n",
       "25000  Working with one of the best Shakespeare sources, this film manages to be creditable to it's source, whilst still appealing to a wider audience.<br /><br />Branagh steals the film from under Fishburne's nose, and there's a talented cast on good form.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "25001  Well...tremors I, the original started off in 1990 and i found the movie quite enjoyable to watch. however, they proceeded to make tremors II and III. Trust me, those movies started going downhill right after they finished the first one, i mean, ass blasters??? Now, only God himself is capable of answering the question \"why in Gods name would they create another one of these dumpster dives of a movie?\" Tremors IV cannot be considered a bad movie, in fact it cannot be even considered an epitome of a bad movie, for it lives up to more than that. As i attempted to sit though it, i noticed that my eyes started to bleed, and i hoped profusely that the little girl from the ring would crawl through the TV and kill me. did they really think that dressing the people who had stared in the other movies up as though they we're from the wild west would make the movie (with the exact same occurrences) any better? honestly, i would never suggest buying this movie, i mean, there are cheaper ways to find things that burn well.  \n",
       "25002  Ouch! This one was a bit painful to sit through. It has a cute and amusing premise, but it all goes to hell from there. Matthew Modine is almost always pedestrian and annoying, and he does not disappoint in this one. Deborah Kara Unger and John Neville turned in surprisingly decent performances. Alan Bates and Jennifer Tilly, among others, played it way over the top. I know that's the way the parts were written, and it's hard to blame actors, when the script and director have them do such schlock. If you're going to have outrageous characters, that's OK, but you gotta have good material to make it work. It didn't here. Run away screaming from this movie if at all possible.                                                                                                                                                                                                                                                                                                                                                          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at some bad reviews\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df[df[\"class\"] == 0].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina's pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles &amp; Beverly D'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary's Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  \\\n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \n",
       "0  For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  Bizarre horror movie filled with famous faces but stolen by Cristina Raines (later of TV's \"Flamingo Road\") as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the Gateway to Hell! The scenes with Raines modeling are very well captured, the mood music is perfect, Deborah Raffin is charming as Cristina's pal, but when Raines moves into a creepy Brooklyn Heights brownstone (inhabited by a blind priest on the top floor), things really start cooking. The neighbors, including a fantastically wicked Burgess Meredith and kinky couple Sylvia Miles & Beverly D'Angelo, are a diabolical lot, and Eli Wallach is great fun as a wily police detective. The movie is nearly a cross-pollination of \"Rosemary's Baby\" and \"The Exorcist\"--but what a combination! Based on the best-seller by Jeffrey Konvitz, \"The Sentinel\" is entertainingly spooky, full of shocks brought off well by director Michael Winner, who mounts a thoughtfully downbeat ending with skill. ***1/2 from ****  \n",
       "2  A solid, if unremarkable film. Matthau, as Einstein, was wonderful. My favorite part, and the only thing that would make me go out of my way to see this again, was the wonderful scene with the physicists playing badmitton, I loved the sweaters and the conversation while they waited for Robbins to retrieve the birdie.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good reviews.\n",
    "df[df[\"class\"] == 1].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_document(document):\n",
    "    # Normalize the text.\n",
    "    document = document.lower()\n",
    "\n",
    "    # Remove non-alphanumeric characters.\n",
    "    document = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", document)\n",
    "    \n",
    "    # Remove sequences of whitespace.\n",
    "    document = re.sub(r\" +\", r\" \", document)\n",
    "    \n",
    "    # Strip trailing newline characters.\n",
    "    document = document.strip()\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(pre_process_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagine a movie where joe piscopo is actually funny maureen stapleton is a scene stealer the moroni character is an absolute scream watch for alan the skipper hale jr as a police sgt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>bizarre horror movie filled with famous faces but stolen by cristina raines later of tvs flamingo road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the gateway to hell the scenes with raines modeling are very well captured the mood music is perfect deborah raffin is charming as cristinas pal but when raines moves into a creepy brooklyn heights brownstone inhabited by a blind priest on the top floor things really start cooking the neighbors including a fantastically wicked burgess meredith and kinky couple sylvia miles beverly dangelo are a diabolical lot and eli wallach is great fun as a wily police detective the movie is nearly a crosspollination of rosemarys baby and the exorcistbut what a combination based on the bestseller by jeffrey konvitz the sentinel is entertainingly spooky full of shocks brought off well by director michael winner who mounts a thoughtfully downbeat ending with skill 12 from</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>a solid if unremarkable film matthau as einstein was wonderful my favorite part and the only thing that would make me go out of my way to see this again was the wonderful scene with the physicists playing badmitton i loved the sweaters and the conversation while they waited for robbins to retrieve the birdie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  \\\n",
       "0  1       \n",
       "1  1       \n",
       "2  1       \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \n",
       "0  for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagine a movie where joe piscopo is actually funny maureen stapleton is a scene stealer the moroni character is an absolute scream watch for alan the skipper hale jr as a police sgt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "1  bizarre horror movie filled with famous faces but stolen by cristina raines later of tvs flamingo road as a pretty but somewhat unstable model with a gummy smile who is slated to pay for her attempted suicides by guarding the gateway to hell the scenes with raines modeling are very well captured the mood music is perfect deborah raffin is charming as cristinas pal but when raines moves into a creepy brooklyn heights brownstone inhabited by a blind priest on the top floor things really start cooking the neighbors including a fantastically wicked burgess meredith and kinky couple sylvia miles beverly dangelo are a diabolical lot and eli wallach is great fun as a wily police detective the movie is nearly a crosspollination of rosemarys baby and the exorcistbut what a combination based on the bestseller by jeffrey konvitz the sentinel is entertainingly spooky full of shocks brought off well by director michael winner who mounts a thoughtfully downbeat ending with skill 12 from  \n",
       "2  a solid if unremarkable film matthau as einstein was wonderful my favorite part and the only thing that would make me go out of my way to see this again was the wonderful scene with the physicists playing badmitton i loved the sweaters and the conversation while they waited for robbins to retrieve the birdie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test_val = train_test_split(df, test_size=0.4)\n",
    "val, test = train_test_split(test_val, test_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 2), (4000, 2), (16000, 2))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = train[\"text\"], val[\"text\"], test[\"text\"]\n",
    "y_train, y_val, y_test = train[\"class\"], val[\"class\"], test[\"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "vocab_size = 3000\n",
    "\n",
    "# Stanford was nice enough to give us the vocab.\n",
    "with open(\"../data/aclimdb/imdb.vocab\", \"r\") as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip())\n",
    "\n",
    "# Map each vocabulary item to an integer between 1 and vocab_size\n",
    "vocab_vector_map = {}\n",
    "for index, word in enumerate(vocab[:vocab_size]): # only use the 4000 most common words\n",
    "    vocab_vector_map[word] = index + 1 # reserve 0 for padding\n",
    "    \n",
    "def map_vectors(reviews):\n",
    "    mapped_reviews = []\n",
    "    \n",
    "    for review in reviews:\n",
    "        mapped_review = []\n",
    "        for word in review.split(\" \"):\n",
    "            try:\n",
    "                mapped_review.append(vocab_vector_map[word])\n",
    "            except:\n",
    "                pass\n",
    "        mapped_reviews.append(mapped_review)\n",
    "    return mapped_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the words in each document to its numerical representation.\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "max_words = 300\n",
    "X_train2 = sequence.pad_sequences(map_vectors(X_train), maxlen=max_words)\n",
    "X_val2 = sequence.pad_sequences(map_vectors(X_val), maxlen=max_words)\n",
    "X_test2 = sequence.pad_sequences(map_vectors(X_test), maxlen=max_words)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  19, 1273,   11,  216,    8,  161,   14,  447,    1,    6,   80,\n",
       "        108,    8,  321, 2179,    2,    1,  125,   38,    6,    1,  103,\n",
       "         33,  445, 2996,   31,    1,  330,    4,    3,   40,   13,  249,\n",
       "          8,    8,  321,    2,    1,  321,    7,   67,  543,    3,  481,\n",
       "        527,   37,   19,   72, 1568,   17,    8,    1,  202,  930,    1,\n",
       "       2052,  769,  401,    1, 1277,   29,   23,    1,  212, 2912,  712,\n",
       "         12,  109,   31,  470,   10,    6,   85, 1459,   22,   59,  138,\n",
       "         19,    5,  301,   13,   27,    4,    1,   43,   20,    1,    4,\n",
       "          1,  427,    1, 2064, 2054,  123,   22,    6,   15, 2098,   34,\n",
       "         59,   26,   19,  942,    2,    1, 2064,   38,    6,   48,   84,\n",
       "       2725,  223,    8,    1,  376,    8,   27,  316,    1, 1682, 1092,\n",
       "          4,    3,  103,    6,  596,   13,   22,    8,  998,    4,    3,\n",
       "       2848, 2536, 2961,    1,  168,  103,   33,    6,   32, 1615,   44,\n",
       "       1135, 1299,   15,    1, 2274,   19,    1, 1647,    8,   24, 1552,\n",
       "         17,   46,    2,   23,   63,  383,    8,  712,    6, 2124, 1814,\n",
       "          1,  164,   14, 1459,  616,   53,  385,    8,   90,   29,  123,\n",
       "          1,    1,    4,  810,    2,    1,    1, 1995,    4,  620,   51,\n",
       "        987,    8, 1140,    1,    4,  690,  502,    7,    6,    3, 1257,\n",
       "        419,    5,  164, 1126, 1168,    1,  173,   13,   72,   13,    1,\n",
       "        841,  544,    2,    4,    1,  153,    2,    1,  414,  702,   16,\n",
       "          9,  199, 1126,   12,    3,  113, 1507,  322,    6,   20,   57,\n",
       "          5,   73,    2,  160,  257,  130,  179,  792,    8,    3,  166,\n",
       "        136,    6,  646,   49,    2,  176,  298,   39,  382,  133,   15,\n",
       "       2952, 1711,    6,   36,    7,   44,    5,   26,  108,    5,   26,\n",
       "       2339,  154,  133,    6,   51,  259,   99,    1,    5,  862,   87,\n",
       "          7,    5,    3, 1251,  133,   13,   52, 2445,   87,  141,    1,\n",
       "       1884,   15,    3,  613,   30,    3, 1646,   48,  146,   49, 1049,\n",
       "          8,   45,    4], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peek at an encoded document\n",
    "X_train2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'on themes that almost in work for example the is also seen in black angel and the man there is the character who becomes mentally by the death of a or as found in in black and the black it can leave a viewer feeling like on well ground but in the right hands the plots sorry dialogue the narrative all are the point fortunately lady was being by sound this is first noir he would go on to himself as one of the if not the of the style the killers cross here he is with woody they would be on christmas and the killers there is some great storytelling done in the camera in one shot the mental state of a character is shown as he in front of a mirror multiple personalities the same character who is an artist has van self with the hanging on the wall in his apartment but what and are really doing in lady is practically creating the look for noir released very early in its all here the the of atmosphere and the the versions of reality when scott in prison the of light etc it is a terrific picture to look tone aside the cast as well as the subject matter and of the director and the budget b movie i thought tone was a little alan high is not up to much and actually comes off pretty weak in a few scenes is mostly good and quite beautiful her sex scene with cook jr is so it has to be seen to be believed another scene is when goes after the to question him it to a chase scene as she dogs him through the streets with a stop at a station some real good tension in out of'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure the transformation was correct.\n",
    "reversed_map = {v: k for k, v in vocab_vector_map.items()}\n",
    "reversed_map[0] = \"0\"\n",
    "\n",
    "decoded = [reversed_map[i] for i in X_train2[0]]\n",
    "\" \".join(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1: Simple model (RNN)\n",
    "LSTM with embedding layer. Each document is reduced to 300 words. Documents with less than this # are zero-padded. An embedding layer of 32 dimensions is added, so this layer's output is (data, 400, 32). This is then fed to an LSTM with output of (data, 100). Then a dense layer with shape (data, 1) is added (binary classification). Word embeddings are being learned during training, so during classification the embedding layer serves as a pre-processer. In `V2` I will use pretrained embeddings to see if performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 32)           96032     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 149,333\n",
      "Trainable params: 149,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the simple model with our custom embedding layer.\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "embedding_dim=32\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, input_length=max_words))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 230s 475ms/step - loss: 0.5488 - accuracy: 0.6940 - val_loss: 0.3616 - val_accuracy: 0.8455\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36160, saving model to ../models/doc_classif/doc_classif_v1.01-0.85.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v1.01-0.85.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v1.01-0.85.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "469/469 [==============================] - 191s 406ms/step - loss: 0.2991 - accuracy: 0.8811 - val_loss: 0.2975 - val_accuracy: 0.8813\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36160 to 0.29753, saving model to ../models/doc_classif/doc_classif_v1.02-0.88.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v1.02-0.88.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v1.02-0.88.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "469/469 [==============================] - 206s 439ms/step - loss: 0.2727 - accuracy: 0.8914 - val_loss: 0.3235 - val_accuracy: 0.8610\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.29753\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 223s 475ms/step - loss: 0.2489 - accuracy: 0.9024 - val_loss: 0.3253 - val_accuracy: 0.8587\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.29753\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 318s 679ms/step - loss: 0.2376 - accuracy: 0.9067 - val_loss: 0.3363 - val_accuracy: 0.8633\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.29753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe9a0fe9040>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define callbacks.\n",
    "patience = 50\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"../models/doc_classif/doc_classif_v1.{epoch:02d}-{val_accuracy:.2f}.model\",\n",
    "                                   \"val_loss\", verbose=1, save_best_only=True)\n",
    "callbacks = [model_checkpoint, early_stop, reduce_lr]\n",
    " \n",
    "model.fit(X_train2, y_train, validation_data=(X_val2, y_val), batch_size=64, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mdoc_classif_v1.01-0.85.model\u001b[m\u001b[m \u001b[34mdoc_classif_v1.02-0.88.model\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "! ls ../models/doc_classif/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 37s 73ms/step - loss: 0.2912 - accuracy: 0.8821\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# The second epoch of the model (out of 50) was already the best-performing, at 86% accuracy.\n",
    "model = load_model(\"../models/doc_classif/doc_classif_v1.02-0.88.model\")\n",
    "\n",
    "score = model.evaluate(X_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy is: 0.882\n"
     ]
    }
   ],
   "source": [
    "print(f\"The model's accuracy is: {score[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure the embeddings make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out what the word embeddings look like\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[0].output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_test2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a word embedding\n",
    "word_id = reversed_map[X_test2[0, -1]]\n",
    "word_embed_1 = intermediate_output[0, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a',\n",
       " array([-0.0428787 ,  0.04246109, -0.04952083, -0.00935871,  0.01690659,\n",
       "        -0.03302329,  0.02850872, -0.01944215, -0.02780089,  0.01906136,\n",
       "         0.0510727 , -0.02535792,  0.03279592,  0.02701469, -0.02171413,\n",
       "         0.00376951, -0.03193901,  0.00115354, -0.00207962, -0.0245267 ,\n",
       "         0.03997526,  0.00283179, -0.01656234,  0.01373642,  0.01304472,\n",
       "         0.02795264, -0.04893907,  0.05676555, -0.03450371, -0.00905003,\n",
       "        -0.02006874,  0.00302825], dtype=float32))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id, word_embed_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: Simple Model (CNN)\n",
    "Let's see if I can get a higher accuracy with a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 32)           96032     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 148, 32)           5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                24608     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 125,825\n",
      "Trainable params: 125,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, ReLU, MaxPool1D, Dense, Flatten\n",
    "\n",
    "# Define the model hyperparameters.\n",
    "kernel_size = 5\n",
    "stride_size = 2\n",
    "\n",
    "# Define the model.\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, embedding_dim, input_length=max_words))\n",
    "\n",
    "model.add(Conv1D(embedding_dim, kernel_size=kernel_size, activation=\"relu\", strides=stride_size))\n",
    "model.add(MaxPool1D(6))\n",
    "\n",
    "# model.add(Conv1D(embedding_dim, kernel_size=kernel_size, activation=\"relu\", strides=stride_size))\n",
    "# model.add(MaxPool1D(5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(embedding_dim, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "\n",
    "model.add(Dense(1, activation=\"sigmoid\", kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 13s 25ms/step - loss: 0.6221 - accuracy: 0.6139 - val_loss: 0.3361 - val_accuracy: 0.8637\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.33608, saving model to ../models/doc_classif/doc_classif_v2.01-0.86.model\n",
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v2.01-0.86.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v2.01-0.86.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.2975 - accuracy: 0.8843 - val_loss: 0.3305 - val_accuracy: 0.8637\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.33608 to 0.33048, saving model to ../models/doc_classif/doc_classif_v2.02-0.86.model\n",
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v2.02-0.86.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/doc_classif/doc_classif_v2.02-0.86.model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.2509 - accuracy: 0.9058 - val_loss: 0.3749 - val_accuracy: 0.8487\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.33048\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.2106 - accuracy: 0.9280 - val_loss: 0.3770 - val_accuracy: 0.8525\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.33048\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.1652 - accuracy: 0.9499 - val_loss: 0.3957 - val_accuracy: 0.8565\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.33048\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 15s 33ms/step - loss: 0.1337 - accuracy: 0.9665 - val_loss: 0.4491 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.33048\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 17s 35ms/step - loss: 0.0984 - accuracy: 0.9805 - val_loss: 0.5420 - val_accuracy: 0.8397\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.33048\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 16s 33ms/step - loss: 0.0756 - accuracy: 0.9887 - val_loss: 0.6573 - val_accuracy: 0.8328\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.33048\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 14s 30ms/step - loss: 0.0623 - accuracy: 0.9922 - val_loss: 0.6333 - val_accuracy: 0.8338\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.33048\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0483 - accuracy: 0.9968 - val_loss: 0.7225 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.33048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe991a86790>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "             optimizer='adam', \n",
    "             metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks.\n",
    "patience = 50\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "model_checkpoint = ModelCheckpoint(\"../models/doc_classif/doc_classif_v2.{epoch:02d}-{val_accuracy:.2f}.model\",\n",
    "                                   \"val_loss\", verbose=1, save_best_only=True)\n",
    "callbacks = [model_checkpoint, early_stop, reduce_lr]\n",
    " \n",
    "model.fit(X_train2, y_train, validation_data=(X_val2, y_val), batch_size=64, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 2s 4ms/step - loss: 0.7042 - accuracy: 0.8417\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's accuracy is: 0.842\n"
     ]
    }
   ],
   "source": [
    "# This is clearly overfitting!\n",
    "print(f\"The model's accuracy is: {score[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.layers[0].output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_test2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a word embedding\n",
    "word_id = reversed_map[X_test2[0, -1]]\n",
    "word_embed_2 = intermediate_output[0, -2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07320897, -0.05457532,  0.05022319,  0.1117916 , -0.00034211,\n",
       "       -0.04083229, -0.0026621 ,  0.02942782, -0.0960895 , -0.1511858 ,\n",
       "       -0.05323165,  0.03701774,  0.01807283, -0.06696399,  0.02081559,\n",
       "        0.0259468 , -0.01137567,  0.0957579 ,  0.0161357 ,  0.05464319,\n",
       "       -0.0994353 , -0.00317309, -0.04847529, -0.00262528, -0.14018223,\n",
       "       -0.07103251,  0.02541416, -0.1559343 ,  0.13609698,  0.03910544,\n",
       "       -0.03721023, -0.0457418 ], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the embeddings that the RNN and CNN learned are simikar, this should be close to 0. That's not the case!\n",
    "word_embed_2 - word_embed_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: Training an embedding layer using Glove\n",
    "Same as **V1** except that the embeddings are trained using glove. These are then fed to the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3a: Simple model + pretrained embeddings\n",
    "Same as **V1** except that pretrained embeddings are used. Glove embeddings used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3b: Simple model + pretrained embeddings\n",
    "Same as **V1** except that pretrained embeddings are used. Elmo embeddings used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
